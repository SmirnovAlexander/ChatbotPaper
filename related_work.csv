name,tags,date,cited,arXiv,code,blogpost,priority,primary goal,description,datasets,eval,key takeaways,limitations,comments,proposed method,FAQ,Column,Column 1
DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation,"chatbot, dialogpt","November 1, 2019",407,https://arxiv.org/pdf/1911.00536.pdf,https://github.com/microsoft/DialoGPT,https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/,1,conversational neural response generation,GPT-2 trained on dialogue pairs/sessions extracted from Reddit discussion chains,reddit comment chains 2005-2017,"BLEU, Dist-n, Entropy, METEOR, NIST, human eval","addressing generating bland samples problem using Mutual Information Maximization scoring function (generate set of hypotheses using top-K sampling and rerank via separate model that calculates $P($Source$|$Hypothesis$)$) ",single-turn dialogues judgement,,,,"February 3, 2022 1:23 PM","February 4, 2022 6:27 PM"
Towards a Human-like Open-Domain Chatbot,"chatbot, meena","January 27, 2020",384,https://arxiv.org/pdf/2001.09977.pdf,,https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html,1,multi-turn open-domain chatbot,study on evaluating chatbots w/ notes on model and decoding,social media conversations,"SSA, perplexity","- perplexity correlates w/ human likeness
- SSA metric: 
    * Sensibleness: does the answer makes sense
    * Specificity: is the answer related to context
- Evolved Transformer 2.6B
- Sample-and-rank decoding (just plain sample N=20 times w/ T=0.88 and choose most probable sequence, w/ top-40 and T=1 a bit better)",empathy,many details on evaluation (both static and interactive),,,"February 3, 2022 1:23 PM","February 4, 2022 6:30 PM"
Recipes for building an open-domain chatbot,"blenderbot, chatbot","April 28, 2020",246,https://arxiv.org/pdf/2004.13637.pdf,https://parl.ai/projects/recipes/,https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/,1,open-domain chatbot,"3 architectures:
- retriever (rank predefined responces)
- generator (transformer)
- retrieve and refine (append predefined response (from retriever) to context; append knowledge to context)","Blended Skill Talk, ConvAI2, Empathetic Dialogues (ED), Wizard of Wikipedia (WoW), pushshift.io Reddit",ACUTE-Eval,,,"good engineering paper, a lot details on training and generation",,,"February 3, 2022 1:20 PM","February 7, 2022 5:41 PM"
"GPT Understands, Too","LM, prompt","March 18, 2021",64,https://arxiv.org/pdf/2103.10385,,,1,,,,,,,,,,"February 23, 2022 11:14 PM","February 23, 2022 11:16 PM"
Internet-Augmented Dialogue Generation,"blenderbot, chatbot","July 15, 2021",14,https://arxiv.org/pdf/2107.07566.pdf,https://parl.ai/projects/blenderbot2/,https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/,1,,,,,,,,,,"February 3, 2022 1:23 PM","February 3, 2022 3:04 PM"
Beyond Goldfish Memory: Long-Term Open-Domain Conversation,"blenderbot, chatbot","July 15, 2021",12,https://arxiv.org/pdf/2107.07567,https://parl.ai/projects/blenderbot2/,https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/,1,long-term open-domain chatbot,Multi-Session Chat (MSC) dataset,,,,,,,,"February 3, 2022 1:23 PM","February 8, 2022 6:24 PM"
RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling,"chatbot, dialogpt","May 14, 2021",2,https://arxiv.org/pdf/2105.06597,https://github.com/dreasysnail/RetGen,,1,,,,,,,,,,"February 3, 2022 1:23 PM","February 3, 2022 3:15 PM"
"Empirical study on BlenderBot 2.0’s Errors Analysis in terms of
Model, Data and User-Centric Approach","analysis, blenderbot","January 10, 2022",0,https://arxiv.org/pdf/2201.03239.pdf,,,1,,,,,,,,,,"February 9, 2022 3:58 PM","February 9, 2022 3:59 PM"
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"LM, T5","October 23, 2019",2566,https://arxiv.org/pdf/1910.10683,https://github.com/google-research/text-to-text-transfer-transformer,https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html,2,,,,,,,,,,"February 3, 2022 1:23 PM","February 3, 2022 3:20 PM"
Multitask Prompted Training Enables Zero-Shot Task Generalization,"LM, T0","October 15, 2021",10,https://arxiv.org/pdf/2110.08207,https://github.com/bigscience-workshop/promptsource/,,2,,,,,,,,,,"February 3, 2022 1:23 PM","February 3, 2022 3:25 PM"
"Scaling Language Models: Methods, Analysis & Insights from Training Gopher","LM, gopher","December 8, 2021",3,https://arxiv.org/pdf/2112.11446.pdf,,https://deepmind.com/blog/article/language-modelling-at-scale,2,,,,,,,,,,"February 3, 2022 1:11 PM","February 3, 2022 2:00 PM"
Improving language models by retrieving from trillions of tokens,"LM, gopher","December 8, 2021",0,https://arxiv.org/pdf/2112.04426.pdf,,https://deepmind.com/blog/article/language-modelling-at-scale,2,,,,,,,,,,"February 3, 2022 1:11 PM","February 3, 2022 2:47 PM"
GPT2,,,,,,,,,,,,,,,,,"February 11, 2022 11:41 AM","February 11, 2022 11:41 AM"
Transformer,,,,,,,,,,,,,,,,,"February 11, 2022 11:41 AM","February 11, 2022 11:41 AM"